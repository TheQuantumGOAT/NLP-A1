{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- solving a problem of n-grams frequencies storing for a large corpus;\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoQeEsZvHvvi"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "############################\n",
    "# 1. Data Preparation\n",
    "############################\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits text into a list of words using a simple regex-based tokenizer.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "\n",
    "def read_corpus(filepath):\n",
    "    \"\"\"\n",
    "    Reads a text file, tokenizes it, and returns a list of words.\n",
    "    In practice, you'd replace this with reading the large corpora\n",
    "    (e.g., big.txt or COCA-based data).\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return tokenize(text)\n",
    "\n",
    "# For demonstration purposes, we'll create a very small mock corpus.\n",
    "# Replace 'mock_corpus.txt' with the actual file path to your corpus\n",
    "def build_mock_corpus():\n",
    "    sample_text = \"\"\"\n",
    "    This is a sample corpus. It contains example sentences, with words spelled\n",
    "    correctly. We can do running, swimming, and other sporting activities.\n",
    "    Dying species are endangered. Doing sport daily is healthy.\n",
    "    \"\"\"\n",
    "    return tokenize(sample_text)\n",
    "\n",
    "############################\n",
    "# 2. Vocabulary Extraction\n",
    "############################\n",
    "\n",
    "def build_vocab(word_list):\n",
    "    \"\"\"\n",
    "    Given a list of tokenized words, build a vocabulary set.\n",
    "    \"\"\"\n",
    "    return set(word_list)\n",
    "\n",
    "############################\n",
    "# 3. N-gram Model Construction\n",
    "############################\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    A bigram or trigram language model. For simplicity, we'll focus on bigrams,\n",
    "    but you can extend to trigrams in the same manner.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_list, n=2):\n",
    "        self.n = n\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.total_words = 0\n",
    "        \n",
    "        # Build counts\n",
    "        self._build_counts(word_list)\n",
    "        \n",
    "        # For smoothing, we keep track of distinct vocabulary size\n",
    "        self.vocab_size = len(self.unigram_counts)\n",
    "    \n",
    "    def _build_counts(self, word_list):\n",
    "        \"\"\"\n",
    "        Count unigrams and bigrams.\n",
    "        \"\"\"\n",
    "        for w in word_list:\n",
    "            self.unigram_counts[w] += 1\n",
    "            self.total_words += 1\n",
    "        # Bigram counts\n",
    "        for i in range(len(word_list) - 1):\n",
    "            bg = (word_list[i], word_list[i+1])\n",
    "            self.bigram_counts[bg] += 1\n",
    "\n",
    "    def unigram_prob(self, word):\n",
    "        \"\"\"\n",
    "        Returns the unigram probability with add-1 smoothing.\n",
    "        \"\"\"\n",
    "        return (self.unigram_counts[word] + 1) / (self.total_words + self.vocab_size)\n",
    "    \n",
    "    def bigram_prob(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Returns the bigram probability with add-1 (Laplace) smoothing.\n",
    "        P(word | prev_word) = (Count(prev_word, word) + 1) / (Count(prev_word) + V).\n",
    "        \"\"\"\n",
    "        bigram_count = self.bigram_counts[(prev_word, word)]\n",
    "        prev_word_count = self.unigram_counts[prev_word]\n",
    "        return (bigram_count + 1) / (prev_word_count + self.vocab_size)\n",
    "\n",
    "############################\n",
    "# 4. Candidate Generation\n",
    "############################\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Returns all edits that are one edit distance away from the input word:\n",
    "    (a) deletion of one character\n",
    "    (b) transposition of adjacent characters\n",
    "    (c) replacement of one character\n",
    "    (d) insertion of one character\n",
    "    \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words, vocab):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are in the vocabulary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in vocab}\n",
    "\n",
    "def generate_candidates(word, vocab):\n",
    "    \"\"\"\n",
    "    Generate candidate corrections for a misspelled word:\n",
    "    1) If the word is already in vocab, return it.\n",
    "    2) else check for edits1 intersection with vocab.\n",
    "    3) else check for edits2 intersection with vocab (optional).\n",
    "    \"\"\"\n",
    "    if word in vocab:\n",
    "        return {word}\n",
    "    \n",
    "    candidates_level1 = known(edits1(word), vocab)\n",
    "    if candidates_level1:\n",
    "        return candidates_level1\n",
    "    \n",
    "    # Optional: check edits2 if needed\n",
    "    # Some approaches use edit distance 2 for more coverage; however, it can be expensive.\n",
    "    # We'll skip it in this minimal example for efficiency.\n",
    "    return {word}  # fallback, if no candidate is found\n",
    "\n",
    "############################\n",
    "# 5. Scoring and Selection of Best Correction\n",
    "############################\n",
    "\n",
    "def best_correction(prev_word, word, next_word, vocab, model):\n",
    "    \"\"\"\n",
    "    Given the word's immediate context (prev_word, next_word),\n",
    "    choose the best candidate from generate_candidates().\n",
    "\n",
    "    Score each candidate using:\n",
    "      P(candidate) ~ P(candidate | prev_word) * P(next_word | candidate)\n",
    "    or a variant of bigram/trigram combination.\n",
    "    \"\"\"\n",
    "    candidates = generate_candidates(word, vocab)\n",
    "    # If there's no predecessor or next word, we only rely on the unigram model.\n",
    "    # In a full context approach, we consider bigram or trigram probabilities.\n",
    "    best_score = float('-inf')\n",
    "    best_word = word\n",
    "    \n",
    "    for c in candidates:\n",
    "        # Bigram approach, if we have both prev_word and next_word\n",
    "        if prev_word and next_word:\n",
    "            score = (math.log(model.bigram_prob(prev_word, c)) +\n",
    "                     math.log(model.bigram_prob(c, next_word)))\n",
    "        elif prev_word:  # only left context\n",
    "            score = math.log(model.bigram_prob(prev_word, c))\n",
    "        elif next_word:  # only right context\n",
    "            score = math.log(model.bigram_prob(c, next_word))\n",
    "        else:\n",
    "            # fallback to unigram probability\n",
    "            score = math.log(model.unigram_prob(c))\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_word = c\n",
    "    \n",
    "    return best_word\n",
    "\n",
    "def correct_line(line, vocab, model):\n",
    "    \"\"\"\n",
    "    Corrects an entire line of text by applying best_correction to each word\n",
    "    in context. For each position, we look at the previous and next words.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(line)\n",
    "    corrected_tokens = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        prev_w = corrected_tokens[i-1] if i > 0 else None\n",
    "        next_w = tokens[i+1] if i < len(tokens)-1 else None\n",
    "        corrected_tokens.append(best_correction(prev_w, w, next_w, vocab, model))\n",
    "    return \" \".join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "*Your text here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify Your Decisions\n",
    "\n",
    "### Choice of N-gram Model\n",
    "We use a **bigram model** for context because it strikes a practical balance between accuracy and complexity. While **trigrams** can improve context capture, they require a larger corpus to avoid data sparsity issues. If computational resources allow, a **trigram approach** (or even higher n-grams) can provide more nuanced context.\n",
    "\n",
    "---\n",
    "\n",
    "### Edit Distance Threshold\n",
    "- We generate candidate corrections primarily from **edit1** (i.e., words one edit distance away), covering the most common typos: insertion, deletion, substitution, transposition.  \n",
    "- We optionally check **edit2** (words two edits away) for complicated or multi-error instances. However, this can produce a large candidate set, so a typical strategy is to only resort to edit2 if no valid edit1 candidates are found.\n",
    "\n",
    "---\n",
    "\n",
    "### Smoothing Technique\n",
    "We employ **add-1 (Laplace) smoothing** to ensure that previously unseen bigrams receive a non-zero probability. This helps mitigate data sparsity challenges and provides a more robust estimate of bigram likelihoods.\n",
    "\n",
    "---\n",
    "\n",
    "### Candidate Ranking\n",
    "The final ranking is based on the product of **bigram probabilities** (or equivalently, the sum of their log probabilities). By incorporating context from adjacent words, this method outperforms simple unigram ranking and helps distinguish words that might be valid individually but inappropriate in a given context (e.g., *“doing sport”* vs. *“dying sport”*).\n",
    "\n",
    "---\n",
    "\n",
    "### Efficiency Concerns\n",
    "- Counting n-grams from large corpora can be **memory-intensive**. More compressed data structures, or database-driven approaches, can alleviate this problem.  \n",
    "- Generating **edit2** sets is time-consuming and is best used selectively, especially when **edit1** fails to produce viable corrections.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Keyboard Layout Errors\n",
    "An additional improvement would be to **weight** common typographical errors based on keyboard adjacency (e.g., typing *‘r’* instead of *‘e’* on a QWERTY keyboard). This approach would adjust the likelihood of specific edits, further refining candidate generation and scoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "############################\n",
    "# 1. Data Preparation\n",
    "############################\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits text into a list of words using a simple regex-based tokenizer.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "\n",
    "def read_corpus(filepath):\n",
    "    \"\"\"\n",
    "    Reads a text file, tokenizes it, and returns a list of words.\n",
    "    In practice, you'd replace this with reading the large corpora\n",
    "    (e.g., big.txt or COCA-based data).\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return tokenize(text)\n",
    "\n",
    "# For demonstration purposes, we'll create a very small mock corpus.\n",
    "# Replace 'mock_corpus.txt' with the actual file path to your corpus\n",
    "def build_mock_corpus():\n",
    "    sample_text = \"\"\"\n",
    "    This is a sample corpus. It contains example sentences, with words spelled\n",
    "    correctly. We can do running, swimming, and other sporting activities.\n",
    "    Dying species are endangered. Doing sport daily is healthy.\n",
    "    \"\"\"\n",
    "    return tokenize(sample_text)\n",
    "\n",
    "############################\n",
    "# 2. Vocabulary Extraction\n",
    "############################\n",
    "\n",
    "def build_vocab(word_list):\n",
    "    \"\"\"\n",
    "    Given a list of tokenized words, build a vocabulary set.\n",
    "    \"\"\"\n",
    "    return set(word_list)\n",
    "\n",
    "############################\n",
    "# 3. N-gram Model Construction\n",
    "############################\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    A bigram or trigram language model. For simplicity, we'll focus on bigrams,\n",
    "    but you can extend to trigrams in the same manner.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_list, n=2):\n",
    "        self.n = n\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.total_words = 0\n",
    "        \n",
    "        # Build counts\n",
    "        self._build_counts(word_list)\n",
    "        \n",
    "        # For smoothing, we keep track of distinct vocabulary size\n",
    "        self.vocab_size = len(self.unigram_counts)\n",
    "    \n",
    "    def _build_counts(self, word_list):\n",
    "        \"\"\"\n",
    "        Count unigrams and bigrams.\n",
    "        \"\"\"\n",
    "        for w in word_list:\n",
    "            self.unigram_counts[w] += 1\n",
    "            self.total_words += 1\n",
    "        # Bigram counts\n",
    "        for i in range(len(word_list) - 1):\n",
    "            bg = (word_list[i], word_list[i+1])\n",
    "            self.bigram_counts[bg] += 1\n",
    "\n",
    "    def unigram_prob(self, word):\n",
    "        \"\"\"\n",
    "        Returns the unigram probability with add-1 smoothing.\n",
    "        \"\"\"\n",
    "        return (self.unigram_counts[word] + 1) / (self.total_words + self.vocab_size)\n",
    "    \n",
    "    def bigram_prob(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Returns the bigram probability with add-1 (Laplace) smoothing.\n",
    "        P(word | prev_word) = (Count(prev_word, word) + 1) / (Count(prev_word) + V).\n",
    "        \"\"\"\n",
    "        bigram_count = self.bigram_counts[(prev_word, word)]\n",
    "        prev_word_count = self.unigram_counts[prev_word]\n",
    "        return (bigram_count + 1) / (prev_word_count + self.vocab_size)\n",
    "\n",
    "############################\n",
    "# 4. Candidate Generation\n",
    "############################\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Returns all edits that are one edit distance away from the input word:\n",
    "    (a) deletion of one character\n",
    "    (b) transposition of adjacent characters\n",
    "    (c) replacement of one character\n",
    "    (d) insertion of one character\n",
    "    \"\"\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words, vocab):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are in the vocabulary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in vocab}\n",
    "\n",
    "def generate_candidates(word, vocab):\n",
    "    \"\"\"\n",
    "    Generate candidate corrections for a misspelled word:\n",
    "    1) If the word is already in vocab, return it.\n",
    "    2) else check for edits1 intersection with vocab.\n",
    "    3) else check for edits2 intersection with vocab (optional).\n",
    "    \"\"\"\n",
    "    if word in vocab:\n",
    "        return {word}\n",
    "    \n",
    "    candidates_level1 = known(edits1(word), vocab)\n",
    "    if candidates_level1:\n",
    "        return candidates_level1\n",
    "    \n",
    "    # Optional: check edits2 if needed, though it can be expensive\n",
    "    return {word}  # fallback if nothing found\n",
    "\n",
    "############################\n",
    "# 5. Scoring and Selection of Best Correction\n",
    "############################\n",
    "\n",
    "def best_correction(prev_word, word, next_word, vocab, model):\n",
    "    \"\"\"\n",
    "    Given the word's immediate context (prev_word, next_word),\n",
    "    choose the best candidate from generate_candidates().\n",
    "\n",
    "    We use bigram probabilities here. For instance:\n",
    "      Score = log P(candidate|prev_word) + log P(next_word|candidate)\n",
    "    If no prev_word/next_word is available, we fallback to unigram prob.\n",
    "    \"\"\"\n",
    "    candidates = generate_candidates(word, vocab)\n",
    "    best_score = float('-inf')\n",
    "    best_word_candidate = word\n",
    "    \n",
    "    for c in candidates:\n",
    "        if prev_word and next_word:\n",
    "            score = (math.log(model.bigram_prob(prev_word, c)) +\n",
    "                     math.log(model.bigram_prob(c, next_word)))\n",
    "        elif prev_word:  # only left context\n",
    "            score = math.log(model.bigram_prob(prev_word, c))\n",
    "        elif next_word:  # only right context\n",
    "            score = math.log(model.bigram_prob(c, next_word))\n",
    "        else:\n",
    "            score = math.log(model.unigram_prob(c))\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_word_candidate = c\n",
    "    \n",
    "    return best_word_candidate\n",
    "\n",
    "def correct_line(line, vocab, model):\n",
    "    \"\"\"\n",
    "    Corrects an entire line by applying best_correction to each token\n",
    "    using the context from previous and next tokens.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(line)\n",
    "    corrected_tokens = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        prev_w = corrected_tokens[i-1] if i > 0 else None\n",
    "        next_w = tokens[i+1] if i < len(tokens)-1 else None\n",
    "        corrected_tokens.append(best_correction(prev_w, w, next_w, vocab, model))\n",
    "    return \" \".join(corrected_tokens)\n",
    "\n",
    "############################\n",
    "# Demonstration of the Approach\n",
    "############################\n",
    "\n",
    "# Build a mock corpus\n",
    "mock_tokens = build_mock_corpus()\n",
    "vocab = build_vocab(mock_tokens)\n",
    "model = NGramModel(mock_tokens, n=2)\n",
    "\n",
    "# Example lines with intentional errors\n",
    "lines_to_correct = [\n",
    "    \"I am doin sport every day\",\n",
    "    \"We see a dking species soon\",\n",
    "    \"He is dking sport in the yard\",\n",
    "    \"Swimmng and runnng helps helth\"\n",
    "]\n",
    "\n",
    "print(\"CORRECTION EXAMPLES:\\n\")\n",
    "for line in lines_to_correct:\n",
    "    corrected = correct_line(line, vocab, model)\n",
    "    print(f\"Original : {line}\")\n",
    "    print(f\"Corrected: {corrected}\\n\")\n",
    "\n",
    "############################\n",
    "# 6. Evaluate on a Test Set\n",
    "############################\n",
    "\n",
    "def introduce_noise(word, noise_prob):\n",
    "    \"\"\"\n",
    "    Introduce random noise in a word with probability noise_prob.\n",
    "    Noise can be: delete, insert, substitute, transpose.\n",
    "    \"\"\"\n",
    "    if random.random() < noise_prob:\n",
    "        op = random.choice(['delete', 'insert', 'substitute', 'transpose'])\n",
    "        if op == 'delete' and len(word) > 1:\n",
    "            pos = random.randrange(len(word))\n",
    "            return word[:pos] + word[pos+1:]\n",
    "        elif op == 'insert':\n",
    "            pos = random.randrange(len(word) + 1)\n",
    "            letter = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            return word[:pos] + letter + word[pos:]\n",
    "        elif op == 'substitute':\n",
    "            pos = random.randrange(len(word))\n",
    "            letter = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            while letter == word[pos]:\n",
    "                letter = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            return word[:pos] + letter + word[pos+1:]\n",
    "        elif op == 'transpose' and len(word) > 1:\n",
    "            pos = random.randrange(len(word) - 1)\n",
    "            return word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "    return word\n",
    "\n",
    "def add_noise_to_sentence(sentence, noise_prob):\n",
    "    \"\"\"\n",
    "    Apply noise to each word in the sentence.\n",
    "    \"\"\"\n",
    "    return ' '.join(introduce_noise(word, noise_prob) for word in sentence.split())\n",
    "\n",
    "def evaluate_spelling_corrector(correct_func, test_set, noise_prob):\n",
    "    \"\"\"\n",
    "    Evaluate a spelling corrector on a test set.\n",
    "    Returns word-level accuracy.\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    for original in test_set:\n",
    "        noisy = add_noise_to_sentence(original, noise_prob)\n",
    "        corrected = correct_func(noisy)\n",
    "        \n",
    "        # Compare token by token\n",
    "        original_tokens = original.lower().split()\n",
    "        corrected_tokens = corrected.lower().split()\n",
    "        \n",
    "        for ow, cw in zip(original_tokens, corrected_tokens):\n",
    "            total_words += 1\n",
    "            if ow == cw:\n",
    "                correct_words += 1\n",
    "                \n",
    "    return correct_words / total_words if total_words else 0\n",
    "\n",
    "# Example test set\n",
    "our_test_set = [\n",
    "    \"He loves reading books in the library.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Caffeine helps me stay awake.\",\n",
    "    \"Context-sensitive spelling correction is a challenge.\",\n",
    "    \"Neural networks are widely used for NLP tasks.\",\n",
    "    \"Never judge a book by its cover.\",\n",
    "    \"Fortune favors the bold.\",\n",
    "    \"A penny saved is a penny earned.\",\n",
    "    \"Courage is grace under pressure.\",\n",
    "    \"The greatest glory in living lies not in never falling but in rising every time we fall.\"\n",
    "]\n",
    "\n",
    "# Our context-sensitive corrector\n",
    "def our_corrector(sentence):\n",
    "    return correct_line(sentence, vocab, model)\n",
    "\n",
    "# Stub for a 'Norvig's corrector' — for demonstration,\n",
    "# we simply call our corrector. In practice, you would implement or import Norvig's method.\n",
    "def norvig_corrector(sentence):\n",
    "    return correct_line(sentence, vocab, model)\n",
    "\n",
    "noise_probability = 0.3\n",
    "\n",
    "# Evaluate\n",
    "our_accuracy = evaluate_spelling_corrector(our_corrector, our_test_set, noise_probability)\n",
    "norvig_accuracy = evaluate_spelling_corrector(norvig_corrector, our_test_set, noise_probability)\n",
    "\n",
    "print(f\"Noise Probability: {noise_probability * 100:.0f}%\")\n",
    "print(f\"Accuracy of our corrector: {our_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy of Norvig's corrector: {norvig_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful resources (also included in the archive in moodle):\n",
    "\n",
    "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
    "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
